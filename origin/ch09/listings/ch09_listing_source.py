모모
모모import binascii
모모import bisect
모모from datetime import date, timedelta
모모from collections import defaultdict
모모import math
모모import time
모모import unittest
모모import uuid
모모
모모import redis
모모
모모def readblocks(conn, key, blocksize=2**17):
모모    lb = blocksize
모모    pos = 0
모모    while lb == blocksize:                                  #A
모모        block = conn.substr(key, pos, pos + blocksize - 1)  #B
모모        yield block                                         #C
모모        lb = len(block)                                     #C
모모        pos += lb                                           #C
모모    yield ''
모모
모모'''
모모# <start id="ziplist-configuration-options"/>
모모list-max-ziplist-entries 512    #A
모모list-max-ziplist-value 64       #A
모모
모모hash-max-ziplist-entries 512    #B
모모hash-max-ziplist-value 64       #B
모모
모모zset-max-ziplist-entries 128    #C
모모zset-max-ziplist-value 64       #C
모모# <end id="ziplist-configuration-options"/>
모모#A Limits for ziplist use with LISTs
모모#B Limits for ziplist use with HASHes (previous versions of Redis used a different name and encoding for this)
모모#C Limits for ziplist use with ZSETs
모모#END
모모'''
모모
모모'''
모모# <start id="ziplist-test"/>
모모>>> conn.rpush('test', 'a', 'b', 'c', 'd')  #A
모모4                                           #A
모모>>> conn.debug_object('test')                                       #B
모모{'encoding': 'ziplist', 'refcount': 1, 'lru_seconds_idle': 20,      #C
모모'lru': 274841, 'at': '0xb6c9f120', 'serializedlength': 24,          #C
모모'type': 'Value'}                                                    #C
모모>>> conn.rpush('test', 'e', 'f', 'g', 'h')  #D
모모8                                           #D
모모>>> conn.debug_object('test')
모모{'encoding': 'ziplist', 'refcount': 1, 'lru_seconds_idle': 0,   #E
모모'lru': 274846, 'at': '0xb6c9f120', 'serializedlength': 36,      #E
모모'type': 'Value'}
모모>>> conn.rpush('test', 65*'a')          #F
모모9
모모>>> conn.debug_object('test')
모모{'encoding': 'linkedlist', 'refcount': 1, 'lru_seconds_idle': 10,   #F
모모'lru': 274851, 'at': '0xb6c9f120', 'serializedlength': 30,          #G
모모'type': 'Value'}
모모>>> conn.rpop('test')                                               #H
모모'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'
모모>>> conn.debug_object('test')
모모{'encoding': 'linkedlist', 'refcount': 1, 'lru_seconds_idle': 0,    #H
모모'lru': 274853, 'at': '0xb6c9f120', 'serializedlength': 17,
모모'type': 'Value'}
모모# <end id="ziplist-test"/>
모모#A Let's start by pushing 4 items onto a LIST
모모#B We can discover information about a particular object with the 'debug object' command
모모#C The information we are looking for is the 'encoding' information, which tells us that this is a ziplist, which is using 24 bytes of memory
모모#D Let's push 4 more items onto the LIST
모모#E We still have a ziplist, and its size grew to 36 bytes (which is exactly 2 bytes overhead, 1 byte data, for each of the 4 items we just pushed)
모모#F When we push an item bigger than what was allowed for the encoding, the LIST gets converted from the ziplist encoding to a standard linked list
모모#G While the serialized length went down, for non-ziplist encodings (except for the special encoding for SETs), this number doesn't represent the amount of actual memory used by the structure
모모#H After a ziplist is converted to a regular structure, it doesn't get re-encoded as a ziplist if the structure later meets the criteria
모모#END
모모'''
모모
모모'''
모모# <start id="intset-configuration-option"/>
모모set-max-intset-entries 512      #A
모모# <end id="intset-configuration-option"/>
모모#A Limits for intset use with SETs
모모#END
모모'''
모모
모모'''
모모# <start id="intset-test"/>
모모>>> conn.sadd('set-object', *range(500))    #A
모모500
모모>>> conn.debug_object('set-object')         #A
모모{'encoding': 'intset', 'refcount': 1, 'lru_seconds_idle': 0,    #A
모모'lru': 283116, 'at': '0xb6d1a1c0', 'serializedlength': 1010,
모모'type': 'Value'}
모모>>> conn.sadd('set-object', *range(500, 1000))  #B
모모500
모모>>> conn.debug_object('set-object')             #B
모모{'encoding': 'hashtable', 'refcount': 1, 'lru_seconds_idle': 0, #B
모모'lru': 283118, 'at': '0xb6d1a1c0', 'serializedlength': 2874,
모모'type': 'Value'}
모모# <end id="intset-test"/>
모모#A Let's add 500 items to the set and see that it is still encoded as an intset
모모#B But when we push it over our configured 512 item limit, the intset is translated into a hash table representation
모모#END
모모'''
모모
모모# <start id="rpoplpush-benchmark"/>
모모def long_ziplist_performance(conn, key, length, passes, psize): #A
모모    conn.delete(key)                    #B
모모    conn.rpush(key, *range(length))     #C
모모    pipeline = conn.pipeline(False)     #D
모모
모모    t = time.time()                     #E
모모    for p in xrange(passes):            #F
모모        for pi in xrange(psize):        #G
모모            pipeline.rpoplpush(key, key)#H
모모        pipeline.execute()              #I
모모
모모    return (passes * psize) / (time.time() - t or .001) #J
모모# <end id="rpoplpush-benchmark"/>
모모#A We are going to parameterize everything so that we can measure performance in a variety of ways
모모#B Start by deleting the named key to ensure that we only benchmark exactly what we intend to
모모#C Initialize the LIST by pushing our desired count of numbers onto the right end
모모#D Prepare a pipeline so that we are less affected by network round-trip times
모모#E Start the timer
모모#F We will perform a number of pipeline executions provided by 'passes'
모모#G Each pipeline execution will include 'psize' actual calls to RPOPLPUSH
모모#H Each call will result in popping the rightmost item from the LIST, pushing to the left end of the same LIST
모모#I Execute the 'psize' calls to RPOPLPUSH
모모#J Calculate the number of calls per second that are performed
모모#END
모모
모모'''
모모# <start id="rpoplpush-performance"/>
모모>>> long_ziplist_performance(conn, 'list', 1, 1000, 100)        #A
모모52093.558416505381                                              #A
모모>>> long_ziplist_performance(conn, 'list', 100, 1000, 100)      #A
모모51501.154762768667                                              #A
모모>>> long_ziplist_performance(conn, 'list', 1000, 1000, 100)     #A
모모49732.490843316067                                              #A
모모>>> long_ziplist_performance(conn, 'list', 5000, 1000, 100)     #B
모모43424.056529592635                                              #B
모모>>> long_ziplist_performance(conn, 'list', 10000, 1000, 100)    #B
모모36727.062573334966                                              #B
모모>>> long_ziplist_performance(conn, 'list', 50000, 1000, 100)    #C
모모16695.140684975777                                              #C
모모>>> long_ziplist_performance(conn, 'list', 100000, 500, 100)    #D
모모553.10821080054586                                              #D
모모# <end id="rpoplpush-performance"/>
모모#A With lists encoded as ziplists at 1000 entries or smaller, Redis is still able to perform around 50,000 operations per second or better
모모#B But as lists encoded as ziplists grow to 5000 or more, performance starts to drop off as memory copy costs start reducing performance
모모#C Once we hit 50,000 entries in a ziplist, performance has dropped significantly
모모#D And once we hit 100,000 entries, ziplists are effectively unusable
모모#END
모모'''
모모
모모def long_ziplist_index(conn, key, length, passes, psize): #A
모모    conn.delete(key)                    #B
모모    conn.rpush(key, *range(length))     #C
모모    length >>= 1
모모    pipeline = conn.pipeline(False)     #D
모모    t = time.time()                     #E
모모    for p in xrange(passes):            #F
모모        for pi in xrange(psize):        #G
모모            pipeline.lindex(key, length)#H
모모        pipeline.execute()              #I
모모    return (passes * psize) / (time.time() - t or .001) #J
모모
모모def long_intset_performance(conn, key, length, passes, psize): #A
모모    conn.delete(key)                    #B
모모    conn.sadd(key, *range(1000000, 1000000+length))     #C
모모    cur = 1000000-1
모모    pipeline = conn.pipeline(False)     #D
모모    t = time.time()                     #E
모모    for p in xrange(passes):            #F
모모        for pi in xrange(psize):        #G
모모            pipeline.spop(key)#H
모모            pipeline.sadd(key, cur)
모모            cur -= 1
모모        pipeline.execute()              #I
모모    return (passes * psize) / (time.time() - t or .001) #J
모모
모모
모모# <start id="calculate-shard-key"/>
모모def shard_key(base, key, total_elements, shard_size):   #A
모모    if isinstance(key, (int, long)) or key.isdigit():   #B
모모        shard_id = int(str(key), 10) // shard_size      #C
모모    else:
모모        shards = 2 * total_elements // shard_size       #D
모모        shard_id = binascii.crc32(key) % shards         #E
모모    return "%s:%s"%(base, shard_id)                     #F
모모# <end id="calculate-shard-key"/>
모모#A We will call the shard_key() function with a base HASH name, along with the key to be stored in the sharded HASH, the total number of expected elements, and the desired shard size
모모#B If the value is an integer or a string that looks like an integer, we will use it directly to calculate the shard id
모모#C For integers, we assume they are sequentially assigned ids, so we can choose a shard id based on the upper 'bits' of the numeric id itself. We also use an explicit base here (necessitating the str() call) so that a key of '010' turns into 10, and not 8
모모#D For non-integer keys, we first calculate the total number of shards desired, based on an expected total number of elements and desired shard size
모모#E When we know the number of shards we want, we hash the key and find its value modulo the number of shards we want
모모#F Finally, we combine the base key with the shard id we calculated to determine the shard key
모모#END
모모
모모# <start id="sharded-hset-hget"/>
모모def shard_hset(conn, base, key, value, total_elements, shard_size):
모모    shard = shard_key(base, key, total_elements, shard_size)    #A
모모    return conn.hset(shard, key, value)                         #B
모모
모모def shard_hget(conn, base, key, total_elements, shard_size):
모모    shard = shard_key(base, key, total_elements, shard_size)    #C
모모    return conn.hget(shard, key)                                #D
모모# <end id="sharded-hset-hget"/>
모모#A Calculate the shard to store our value in
모모#B Set the value in the shard
모모#C Calculate the shard to fetch our value from
모모#D Get the value in the shard
모모#END
모모
모모'''
모모# <start id="sharded-ip-lookup"/>
모모TOTAL_SIZE = 320000                                             #A
모모SHARD_SIZE = 1024                                               #A
모모
모모def import_cities_to_redis(conn, filename):
모모    for row in csv.reader(open(filename)):
모모        ...
모모        shard_hset(conn, 'cityid2city:', city_id,               #B
모모            json.dumps([city, region, country]),                #B
모모            TOTAL_SIZE, SHARD_SIZE)                             #B
모모
모모def find_city_by_ip(conn, ip_address):
모모    ...
모모    data = shard_hget(conn, 'cityid2city:', city_id,            #C
모모        TOTAL_SIZE, SHARD_SIZE)                                 #C
모모    return json.loads(data)
모모# <end id="sharded-ip-lookup"/>
모모#A We set the arguments for the sharded calls as global constants to ensure that we always pass the same information
모모#B To set the data, we need to pass the TOTAL_SIZE and SHARD_SIZE information, though in this case TOTAL_SIZE is unused because our ids are numeric
모모#C To fetch the data, we need to use the same information for TOTAL_SIZE and SHARD_SIZE for general sharded keys
모모#END
모모'''
모모
모모# <start id="sharded-sadd"/>
모모def shard_sadd(conn, base, member, total_elements, shard_size):
모모    shard = shard_key(base,
모모        'x'+str(member), total_elements, shard_size)            #A
모모    return conn.sadd(shard, member)                             #B
모모# <end id="sharded-sadd"/>
모모#A Shard the member into one of the sharded SETs, remember to turn it into a string because it isn't a sequential id
모모#B Actually add the member to the shard
모모#END
모모
모모# <start id="unique-visitor-count"/>
모모SHARD_SIZE = 512                        #B
모모
모모def count_visit(conn, session_id):
모모    today = date.today()                                #C
모모    key = 'unique:%s'%today.isoformat()                 #C
모모    expected = get_expected(conn, key, today)           #D
모모 
모모    id = int(session_id.replace('-', '')[:15], 16)      #E
모모    if shard_sadd(conn, key, id, expected, SHARD_SIZE): #F
모모        conn.incr(key)                                  #G
모모# <end id="unique-visitor-count"/>
모모#B And we stick with a typical shard size for the intset encoding for SETs
모모#C Get today's date and generate the key for the unique count
모모#D Fetch or calculate the expected number of unique views today
모모#E Calculate the 56 bit id for this 128 bit UUID
모모#F Add the id to the sharded SET
모모#G If the id wasn't in the sharded SET, then we increment our uniqie view count
모모#END
모모
모모# <start id="expected-viewer-count"/>
모모DAILY_EXPECTED = 1000000                                #I
모모EXPECTED = {}                                           #A
모모
모모def get_expected(conn, key, today):
모모    if key in EXPECTED:                                 #B
모모        return EXPECTED[key]                            #B
모모 
모모    exkey = key + ':expected'
모모    expected = conn.get(exkey)                          #C
모모 
모모    if not expected:
모모        yesterday = (today - timedelta(days=1)).isoformat() #D
모모        expected = conn.get('unique:%s'%yesterday)          #D
모모        expected = int(expected or DAILY_EXPECTED)          #D
모모 
모모        expected = 2**int(math.ceil(math.log(expected*1.5, 2))) #E
모모        if not conn.setnx(exkey, expected):                 #F
모모            expected = conn.get(exkey)                      #G
모모 
모모    EXPECTED[key] = int(expected)                       #H
모모    return EXPECTED[key]                                #H
모모# <end id="expected-viewer-count"/>
모모#I We start with an initial expected number of daily visits that may be a little high
모모#A Keep a local copy of any calculated expected counts
모모#B If we have already calculated or seen the expected number of views for today, use that number
모모#C If someone else has already calculated the expected number of views for today, use that number
모모#D Fetch the unique count for yesterday, or if not available, use our default 1 million
모모#E Add 50% to yesterday's count, and round up to the next even power of 2, under the assumption that view count today should be at least 50% better than yesterday
모모#F Save our calculated expected number of views back to Redis for other calls if possible
모모#G If someone else stored the expected count for today before us, use their count instead
모모#H Keep a local copy of today's expected number of hits, and return it back to the caller
모모#END
모모
모모# <start id="location-tables"/>
모모COUNTRIES = '''
모모ABW AFG AGO AIA ALA ALB AND ARE ARG ARM ASM ATA ATF ATG AUS AUT AZE BDI
모모BEL BEN BES BFA BGD BGR BHR BHS BIH BLM BLR BLZ BMU BOL BRA BRB BRN BTN
모모BVT BWA CAF CAN CCK CHE CHL CHN CIV CMR COD COG COK COL COM CPV CRI CUB
모모CUW CXR CYM CYP CZE DEU DJI DMA DNK DOM DZA ECU EGY ERI ESH ESP EST ETH
모모FIN FJI FLK FRA FRO FSM GAB GBR GEO GGY GHA GIB GIN GLP GMB GNB GNQ GRC
모모GRD GRL GTM GUF GUM GUY  HMD HND HRV HTI HUN IDN IMN IND IOT IRL IRN
모모IRQ ISL ISR ITA JAM JEY JOR JPN KAZ KEN KGZ KHM KIR KNA KOR KWT LAO LBN
모모LBR LBY LCA LIE LKA LSO LTU LUX LVA MAC MAF MAR MCO MDA MDG MDV MEX MHL
모모MKD MLI MLT MMR MNE MNG MNP MOZ MRT MSR MTQ MUS MWI MYS MYT NAM NCL NER
모모NFK NGA NIC NIU NLD NOR NPL NRU NZL OMN PAK PAN PCN PER PHL PLW PNG POL
모모PRI PRK PRT PRY PSE PYF QAT REU ROU RUS RWA SAU SDN SEN SGP SGS SHN SJM
모모SLB SLE SLV SMR SOM SPM SRB SSD STP SUR SVK SVN SWE SWZ SXM SYC SYR TCA
모모TCD TGO THA TJK TKL TKM TLS TON TTO TUN TUR TUV TZA UGA UKR UMI URY
모모USA UZB VAT VCT VEN VGB VIR VNM VUT WLF WSM YEM ZAF ZMB ZWE'''.split()#A
모모
모모STATES = {
모모    'CAN':'''AB BC MB NB NL NS NT NU ON PE QC SK YT'''.split(),       #B
모모    'USA':'''AA AE AK AL AP AR AS AZ CA CO CT DC DE FL FM GA GU HI IA ID
모모IL IN KS KY LA MA MD ME MH MI MN MO MP MS MT NC ND NE NH NJ NM NV NY OH
모모OK OR PA PR PW RI SC SD TN TX UT VA VI VT WA WI WV WY'''.split(),     #C
모모}
모모# <end id="location-tables"/>
모모#A A table of ISO 3 country codes. Calling 'split()' will split the string on whitespace, turning the string into a list of country codes
모모#B Province/territory information for Canada
모모#C State information for the United States
모모#END
모모
모모# <start id="location-to-code"/>
모모def get_code(country, state):
모모    cindex = bisect.bisect_left(COUNTRIES, country)             #A
모모    if cindex > len(COUNTRIES) or COUNTRIES[cindex] != country: #B
모모        cindex = -1                                             #B
모모    cindex += 1                                                 #C
모모
모모    sindex = -1
모모    if state and country in STATES:
모모        states = STATES[country]                                #D
모모        sindex = bisect.bisect_left(states, state)              #E
모모        if sindex > len(states) or states[sindex] != state:     #F
모모            sindex = -1                                         #F
모모    sindex += 1                                                 #G
모모
모모    return chr(cindex) + chr(sindex)                            #H
모모# <end id="location-to-code"/>
모모#A Find the offset for the country
모모#B If the country isn't found, then set its index to be -1
모모#C Because uninitialized data in Redis will return as nulls, we want 'not found' to be 0, and the first country to be 1
모모#D Pull the state information for the country, if it is available
모모#E Find the offset for the state
모모#F Handle not-found states like we did with countries
모모#G Keep not-found states at 0, and found states > 0
모모#H The chr() function will turn an integer value of 0..255 into the ascii character with that same value
모모#END
모모
모모# <start id="set-location-information"/>
모모USERS_PER_SHARD = 2**20                                     #A
모모
모모def set_location(conn, user_id, country, state):
모모    code = get_code(country, state)                         #B
모모    
모모    shard_id, position = divmod(user_id, USERS_PER_SHARD)   #C
모모    offset = position * 2                                   #D
모모
모모    pipe = conn.pipeline(False)
모모    pipe.setrange('location:%s'%shard_id, offset, code)     #E
모모
모모    tkey = str(uuid.uuid4())                                #F
모모    pipe.zadd(tkey, 'max', user_id)                         #F
모모    pipe.zunionstore('location:max',                        #F
모모        [tkey, 'location:max'], aggregate='max')            #F
모모    pipe.delete(tkey)                                       #F
모모
모모    pipe.execute()
모모# <end id="set-location-information"/>
모모#A Set the size of each shard
모모#B Get the location code to store for the user
모모#C Find the shard id and position of the user in the specific shard
모모#D Calculate the offset of the user's data
모모#E Set the value in the proper sharded location table
모모#F Update a ZSET that stores the maximum user id seen so far
모모#END
모모
모모# <start id="aggregate-population"/>
모모def aggregate_location(conn):
모모    countries = defaultdict(int)                                #A
모모    states = defaultdict(lambda:defaultdict(int))               #A
모모
모모    max_id = int(conn.zscore('location:max', 'max'))            #B
모모    max_block = max_id // USERS_PER_SHARD                       #B
모모
모모    for shard_id in xrange(max_block + 1):                      #C
모모        for block in readblocks(conn, 'location:%s'%shard_id):  #D
모모            for offset in xrange(0, len(block)-1, 2):           #E
모모                code = block[offset:offset+2]
모모                update_aggregates(countries, states, [code])    #F
모모
모모    return countries, states
모모# <end id="aggregate-population"/>
모모#A Initialize two special structures that will allow us to quickly update existing and missing counters quickly
모모#B Fetch the maximum user id known, and use that to calculate the maximum shard id that we need to visit
모모#C Sequentially check every shard
모모#D ... reading each block
모모#E Extract each code from the block and look up the original location information (like USA, CA for someone who lives in California)
모모#F Update our aggregates
모모#END
모모
모모# <start id="code-to-location"/>
모모def update_aggregates(countries, states, codes):
모모    for code in codes:
모모        if len(code) != 2:                              #A
모모            continue                                    #A
모모
모모        country = ord(code[0]) - 1                      #B
모모        state = ord(code[1]) - 1                        #B
모모        
모모        if country < 0 or country >= len(COUNTRIES):    #C
모모            continue                                    #C
모모
모모        country = COUNTRIES[country]                    #D
모모        countries[country] += 1                         #E
모모
모모        if country not in STATES:                       #F
모모            continue                                    #F
모모        if state < 0 or state >= STATES[country]:       #F
모모            continue                                    #F
모모
모모        state = STATES[country][state]                  #G
모모        states[country][state] += 1                     #H
모모# <end id="code-to-location"/>
모모#A Only look up codes that could be valid
모모#B Calculate the actual offset of the country and state in the lookup tables
모모#C If the country is out of the range of valid countries, continue to the next code
모모#D Fetch the ISO-3 country code
모모#E Count this user in the decoded country
모모#F If we don't have state information or if the state is out of the range of valid states for the country, continue to the next code
모모#G Fetch the state name from the code
모모#H Increment the count for the state
모모#END
모모
모모# <start id="aggregate-limited"/>
모모def aggregate_location_list(conn, user_ids):
모모    pipe = conn.pipeline(False)                                 #A
모모    countries = defaultdict(int)                                #B
모모    states = defaultdict(lambda: defaultdict(int))              #B
모모
모모    for i, user_id in enumerate(user_ids):
모모        shard_id, position = divmod(user_id, USERS_PER_SHARD)   #C
모모        offset = position * 2                                   #C
모모
모모        pipe.substr('location:%s'%shard_id, offset, offset+1)   #D
모모
모모        if (i+1) % 1000 == 0:                                   #E
모모            update_aggregates(countries, states, pipe.execute())#E
모모
모모    update_aggregates(countries, states, pipe.execute())        #F
모모
모모    return countries, states                                    #G
모모# <end id="aggregate-limited"/>
모모#A Set up the pipeline so that we aren't making too many round-trips to Redis
모모#B Set up our base aggregates as we did before
모모#C Calculate the shard id and offset into the shard for this user's location
모모#D Send another pipelined command to fetch the location information for the user
모모#E Every 1000 requests, we will actually update the aggregates using the helper function we defined before
모모#F Handle the last hunk of users that we might have missed before
모모#G Return the aggregates
모모#END
모모
모모class TestCh09(unittest.TestCase):
모모    def setUp(self):
모모        self.conn = redis.Redis(db=15)
모모        self.conn.flushdb()
모모    def tearDown(self):
모모        self.conn.flushdb()
모모
모모    def test_long_ziplist_performance(self):
모모        long_ziplist_performance(self.conn, 'test', 5, 10, 10)
모모        self.assertEquals(self.conn.llen('test'), 5)
모모
모모    def test_shard_key(self):
모모        base = 'test'
모모        self.assertEquals(shard_key(base, 1, 2, 2), 'test:0')
모모        self.assertEquals(shard_key(base, '1', 2, 2), 'test:0')
모모        self.assertEquals(shard_key(base, 125, 1000, 100), 'test:1')
모모        self.assertEquals(shard_key(base, '125', 1000, 100), 'test:1')
모모
모모        for i in xrange(50):
모모            self.assertTrue(0 <= int(shard_key(base, 'hello:%s'%i, 1000, 100).partition(':')[-1]) < 20)
모모            self.assertTrue(0 <= int(shard_key(base, i, 1000, 100).partition(':')[-1]) < 10)
모모
모모    def test_sharded_hash(self):
모모        for i in xrange(50):
모모            shard_hset(self.conn, 'test', 'keyname:%s'%i, i, 1000, 100)
모모            self.assertEquals(shard_hget(self.conn, 'test', 'keyname:%s'%i, 1000, 100), str(i))
모모            shard_hset(self.conn, 'test2', i, i, 1000, 100)
모모            self.assertEquals(shard_hget(self.conn, 'test2', i, 1000, 100), str(i))
모모
모모    def test_sharded_sadd(self):
모모        for i in xrange(50):
모모            shard_sadd(self.conn, 'testx', i, 50, 50)
모모        self.assertEquals(self.conn.scard('testx:0') + self.conn.scard('testx:1'), 50)
모모
모모    def test_unique_visitors(self):
모모        global DAILY_EXPECTED
모모        DAILY_EXPECTED = 10000
모모        
모모        for i in xrange(179):
모모            count_visit(self.conn, str(uuid.uuid4()))
모모        self.assertEquals(self.conn.get('unique:%s'%(date.today().isoformat())), '179')
모모
모모        self.conn.flushdb()
모모        self.conn.set('unique:%s'%((date.today() - timedelta(days=1)).isoformat()), 1000)
모모        for i in xrange(183):
모모            count_visit(self.conn, str(uuid.uuid4()))
모모        self.assertEquals(self.conn.get('unique:%s'%(date.today().isoformat())), '183')
모모
모모    def test_user_location(self):
모모        i = 0
모모        for country in COUNTRIES:
모모            if country in STATES:
모모                for state in STATES[country]:
모모                    set_location(self.conn, i, country, state)
모모                    i += 1
모모            else:
모모                set_location(self.conn, i, country, '')
모모                i += 1
모모        
모모        _countries, _states = aggregate_location(self.conn)
모모        countries, states = aggregate_location_list(self.conn, range(i+1))
모모        
모모        self.assertEquals(_countries, countries)
모모        self.assertEquals(_states, states)
모모
모모        for c in countries:
모모            if c in STATES:
모모                self.assertEquals(len(STATES[c]), countries[c])
모모                for s in STATES[c]:
모모                    self.assertEquals(states[c][s], 1)
모모            else:
모모                self.assertEquals(countries[c], 1)
모모
모모if __name__ == '__main__':
모모    unittest.main()
모모
모모
모모
모모
모모
모모
모모
모모
모모
모모
모모
모모
모모
